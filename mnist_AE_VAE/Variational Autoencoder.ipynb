{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vital-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joined-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "separated-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and test datasets\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5), (0.5)) #Normalize to (-1,1)\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5), (0.5)) #Normalize to (-1,1)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True,download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False,download=True, transform=transform_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "domestic-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "completed-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc3_mean = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc3_std = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder \n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc3_mean(h), self.fc3_std(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std) #  same size as std, normal distribution with mean 0 and variance 1\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "VAE = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    VAE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "union-america",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3_mean): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc3_std): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc4): Linear(in_features=2, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "legislative-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(VAE.parameters(),\n",
    "                                 lr=0.01, \n",
    "                                 weight_decay=1e-5)\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranking-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    VAE.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(trainloader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = VAE(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coordinate-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    VAE.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in testloader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = VAE(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "remarkable-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\envs\\myclone\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 546.174072\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 171.156006\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 160.211716\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 152.706070\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 161.347153\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 162.375565\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 158.782028\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 149.364517\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 150.578201\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 160.971161\n",
      "====> Epoch: 1 Average loss: 164.7879\n",
      "====> Test set loss: 157.0424\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 158.106506\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 155.973495\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 153.721207\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 159.605270\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 151.420303\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 166.110062\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 156.298416\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 148.580856\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 150.091873\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 154.561096\n",
      "====> Epoch: 2 Average loss: 155.5776\n",
      "====> Test set loss: 153.8267\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 157.504669\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 163.038467\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 160.637833\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 155.932343\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 153.162399\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 159.696854\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 144.497833\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 154.596817\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 153.043106\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 149.845535\n",
      "====> Epoch: 3 Average loss: 153.8724\n",
      "====> Test set loss: 153.9035\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 155.465897\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 143.573822\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 156.534073\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 158.212646\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 148.779541\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 158.005356\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 151.359161\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 157.760788\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 154.566193\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 154.844437\n",
      "====> Epoch: 4 Average loss: 152.1949\n",
      "====> Test set loss: 151.9165\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 151.312271\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 152.305389\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 159.707672\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 146.683868\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 146.733231\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 150.668365\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 148.725449\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 148.397339\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 165.717773\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 165.439941\n",
      "====> Epoch: 5 Average loss: 152.0967\n",
      "====> Test set loss: 153.4471\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 144.577820\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 146.491165\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 152.680573\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 157.254013\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 153.741104\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 139.141586\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 150.549713\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 154.990723\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 141.099457\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 163.088348\n",
      "====> Epoch: 6 Average loss: 151.3643\n",
      "====> Test set loss: 152.2473\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 158.173218\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 154.388336\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 154.727493\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 152.585754\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 154.286743\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 144.344650\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 159.322586\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 156.406738\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 163.605606\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 155.929077\n",
      "====> Epoch: 7 Average loss: 151.4439\n",
      "====> Test set loss: 152.5348\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 144.232315\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 146.422119\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 158.262085\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 155.533432\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 148.067825\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 145.610580\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 156.856720\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 153.211670\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 152.052658\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 150.565186\n",
      "====> Epoch: 8 Average loss: 151.2319\n",
      "====> Test set loss: 151.5938\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 152.840759\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 148.091309\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 150.889481\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 156.850677\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 152.389832\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 151.403458\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 156.227417\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 146.183975\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 146.690765\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 144.716476\n",
      "====> Epoch: 9 Average loss: 153.2802\n",
      "====> Test set loss: 149.8457\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 143.938538\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 151.023560\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 147.404648\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 153.107269\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 130.750366\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 153.968964\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 144.530151\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 155.614334\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 136.326279\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 151.975357\n",
      "====> Epoch: 10 Average loss: 150.2881\n",
      "====> Test set loss: 150.7901\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 150.158661\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 148.481705\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 153.492462\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 167.892273\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 171.367599\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 157.667587\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 152.712051\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 155.797043\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 154.276321\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 155.188889\n",
      "====> Epoch: 11 Average loss: 2307582.1952\n",
      "====> Test set loss: 153.8009\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 155.766235\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 150.755020\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 149.247147\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 157.763611\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 149.250198\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 147.800186\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 145.298203\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 143.670517\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 151.789307\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 151.352371\n",
      "====> Epoch: 12 Average loss: 152.3974\n",
      "====> Test set loss: 152.5232\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 154.366943\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 164.269684\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 150.001526\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 150.114227\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 154.035431\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 152.460754\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 159.705948\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 163.246521\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 155.924759\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 144.248657\n",
      "====> Epoch: 13 Average loss: 151.3103\n",
      "====> Test set loss: 149.8960\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 143.463104\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 155.399109\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 149.731613\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 155.125381\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 153.466263\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 153.785385\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 137.702957\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 148.852112\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 144.274094\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 150.791016\n",
      "====> Epoch: 14 Average loss: 150.5340\n",
      "====> Test set loss: 150.3327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 152.200317\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 145.422150\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 151.635590\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 154.967621\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 141.640335\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 148.948578\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 153.463181\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 139.895462\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 145.863510\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 148.903320\n",
      "====> Epoch: 15 Average loss: 150.4988\n",
      "====> Test set loss: 150.3100\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 146.384979\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 149.890198\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 153.281982\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 152.172440\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 147.262161\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 143.279327\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 153.527664\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 150.749847\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 148.554596\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 155.211426\n",
      "====> Epoch: 16 Average loss: 150.3957\n",
      "====> Test set loss: 151.1401\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 149.904099\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 145.347321\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 143.587036\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 148.876129\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 146.471497\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 145.103561\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 146.203064\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 157.063324\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 152.949326\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 150.990997\n",
      "====> Epoch: 17 Average loss: 150.0816\n",
      "====> Test set loss: 150.9976\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 150.850098\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 158.363480\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 142.936951\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 146.875290\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 141.720413\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 145.253845\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 153.114502\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 149.219437\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 159.756653\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 159.211716\n",
      "====> Epoch: 18 Average loss: 149.7641\n",
      "====> Test set loss: 150.3671\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 149.536621\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 149.112213\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 144.502716\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 148.184662\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 142.997025\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 140.845047\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 147.578796\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 146.482742\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 144.369400\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 144.847626\n",
      "====> Epoch: 19 Average loss: 149.0880\n",
      "====> Test set loss: 151.8194\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 152.176041\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 155.316284\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 155.664780\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 147.364090\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 153.897110\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 145.026001\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 154.108826\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 143.314972\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 149.224686\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 139.813522\n",
      "====> Epoch: 20 Average loss: 150.2669\n",
      "====> Test set loss: 151.6752\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 154.619583\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 144.812592\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 146.049164\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 145.468399\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 145.175323\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 149.950775\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 154.611694\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 143.114471\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 154.552200\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 155.194733\n",
      "====> Epoch: 21 Average loss: 150.1444\n",
      "====> Test set loss: 150.2906\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 136.755035\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 152.399902\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 163.378647\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 148.796356\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 141.500549\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 153.719193\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 154.234665\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 147.102463\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 150.432800\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 135.070312\n",
      "====> Epoch: 22 Average loss: 149.0376\n",
      "====> Test set loss: 151.5449\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 152.994812\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 147.465210\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 149.257523\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 150.178177\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 151.288071\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 153.427521\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 143.732635\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 146.565811\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 148.546799\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 144.071243\n",
      "====> Epoch: 23 Average loss: 149.4238\n",
      "====> Test set loss: 150.1254\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 151.959335\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 165.289124\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 151.213623\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 143.399979\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 149.307007\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 144.123718\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 150.816452\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 156.103745\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 146.492935\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 156.448761\n",
      "====> Epoch: 24 Average loss: 150.1791\n",
      "====> Test set loss: 152.0968\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 146.873001\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 146.435410\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 151.975861\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 142.487549\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 145.707153\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 148.441513\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 152.153107\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 151.359772\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 153.023392\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 161.184540\n",
      "====> Epoch: 25 Average loss: 150.2996\n",
      "====> Test set loss: 150.1554\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 145.830521\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 150.786057\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 151.953217\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 145.266357\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 141.201904\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 153.214264\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 154.424973\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 165.580856\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 151.074280\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 141.387665\n",
      "====> Epoch: 26 Average loss: 150.2376\n",
      "====> Test set loss: 150.3861\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 150.158997\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 154.511459\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 155.234283\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 156.875671\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 157.009583\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 161.297485\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 159.520706\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 147.384277\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 156.719284\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 154.082214\n",
      "====> Epoch: 27 Average loss: 150.6708\n",
      "====> Test set loss: 150.7182\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 152.023148\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 151.157684\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 147.573837\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 151.763062\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 150.916519\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 141.645065\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 136.349564\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 151.708557\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 140.072693\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 158.701065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 28 Average loss: 150.1341\n",
      "====> Test set loss: 150.9705\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 148.369888\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 152.547699\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 151.674133\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 161.132309\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 146.740417\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 155.086121\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 153.117676\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 148.310074\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 151.034760\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 155.261719\n",
      "====> Epoch: 29 Average loss: 150.9881\n",
      "====> Test set loss: 150.5300\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 153.597595\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 157.713623\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 159.840271\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 151.402115\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 149.498840\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 145.843353\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 156.331375\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 152.772598\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 143.837250\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 148.161652\n",
      "====> Epoch: 30 Average loss: 150.1425\n",
      "====> Test set loss: 152.0794\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 31):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-ability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-convergence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
